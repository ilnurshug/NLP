{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель\n",
    "Мне не удалось добится хороших результатов используя данную модель, но тем не менее, идея мне показалась довольно интересной.\n",
    "### Основная идея\n",
    "1. Построить разбиение множества языков, в соответствии, например, со схожестью алфавитов.\n",
    "2. Для каждой группы языков построить классификатор, который хорошо умеет различать языки в рамках данной группы.\n",
    "3. Построить классификатор, который бы в качестве ответа для текста выдавал группу языков. Далее мы бы пользовались классификатором для выбранной группы.\n",
    "\n",
    "### Подготовка данных (для группы языков)\n",
    "Сначала строится алфавит, представляющий из себя объединение алфавитов языков в группе.\n",
    "\n",
    "Для каждого текста выполняются следующие преобразования:\n",
    "\n",
    "1. Из текста удаляются не alphanum символы (включая пробелы).\n",
    "2. Текст переводится в нижний регистр.\n",
    "3. Текст обрезается до определенной длины.\n",
    "4. Для каждого символа в тексте выполняется one-hot-encoding, таким образом из текста мы получаем вектор, состоящий из векторов для символов из текста.\n",
    "\n",
    "### Классификатор (для группы языков)\n",
    "Классификатор представляет из себя RNN с LSTM слоями.\n",
    "\n",
    "# Эксперименты\n",
    "Возникли следующие проблемы при попытке разбить множество языков на группы:\n",
    "1. В data-train есть ошибки, то есть некоторым языкам поставлен в соответствие текст на другом языке. Это приводит к тому, что алфавиты разных групп языков сильно пересекаются.\n",
    "2. Количество текстов в разных группах языков может очень сильно отличаться.\n",
    "\n",
    "В виду данных проблем, была предпринята попытка построить классификатор для группы языков, в которую входят все языки.\n",
    "\n",
    "Возможно, если потратить больше времени на обучение классификатора, то можно было бы добиться хороших результатов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "lines = [line.rstrip('\\n') for line in open('data-train.txt')]\n",
    "\n",
    "def text_filter(text):\n",
    "    return ''.join([c.lower() for c in text if c.isalpha()])\n",
    "\n",
    "l = {}\n",
    "l_cnt = {}\n",
    "\n",
    "for i in range(1, len(lines)):\n",
    "    lang, text = lines[i].split('\\t', 1)\n",
    "    \n",
    "    text = text_filter(text)\n",
    "    \n",
    "    f = l_cnt.get(lang, 0)\n",
    "    l_cnt[lang] = f + 1\n",
    "    \n",
    "    if (l.get(lang) is None):\n",
    "        l[lang] = [text]\n",
    "    else:\n",
    "        l[lang].append(text) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "min_len = int(1e5)\n",
    "\n",
    "for land in l:\n",
    "    for sent in l[lang]:\n",
    "        min_len = min(min_len, len(sent))\n",
    "        \n",
    "print(min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lang in l:\n",
    "    for i in range(len(l[lang])):\n",
    "        l[lang][i] = l[lang][i][:min_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lang_groups = [\n",
    "    ['ru', 'uk', 'be', 'bg', 'mk'],\n",
    "    ['en', 'de', 'fr', 'it', 'la', 'nl', 'pt', 'sv', 'es'],\n",
    "    ['sr', 'sl', 'sk', 'pl', 'cs', 'ro', 'hr', 'hsb', 'lt', 'lv'],\n",
    "    ['hy'],\n",
    "    ['el']\n",
    "]\n",
    "\n",
    "lang_ids = [\n",
    "    {'ru':0, 'uk':1, 'be':2, 'bg':3, 'mk':4},\n",
    "    {'en':0, 'de':1, 'fr':2, 'it':3, 'la':4, 'nl':5, 'pt':6, 'sv':7, 'es':8},\n",
    "    {'sr':0, 'sl':1, 'sk':2, 'pl':3, 'cs':4, 'ro':5, 'hr':6, 'hsb':7, 'lt':8, 'lv':9},\n",
    "    {'hy':0},\n",
    "    {'el':0}\n",
    "]\n",
    "\n",
    "l_arr = ['be','bg','cs','de','el','en','es','fr','hr','hsb','hy','it','la','lt','lv','mk','nl','pl','pt','ro','ru','sk','sl','sr','sv','uk']\n",
    "\n",
    "l_ids = {}\n",
    "l_i = 0\n",
    "l_from_id = {}\n",
    "\n",
    "for lang in l_arr:\n",
    "    l_ids[lang] = l_i\n",
    "    l_from_id[l_i] = lang\n",
    "    l_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def alphabet_of_lang_group(group):\n",
    "    a = set([])\n",
    "    for lang in group:\n",
    "        for sent in l[lang]:\n",
    "            for c in sent:\n",
    "                a.add(c)\n",
    "    alphabet = {}\n",
    "    i = 0\n",
    "    for c in a:\n",
    "        alphabet[c] = i\n",
    "        i += 1\n",
    "    return alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabets = []\n",
    "for group in lang_groups:\n",
    "    alphabets.append(alphabet_of_lang_group(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    }
   ],
   "source": [
    "A = alphabet_of_lang_group(l_arr)\n",
    "\n",
    "print(len(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ru': 173608, 'cs': 1799, 'la': 500, 'it': 3395, 'sk': 1604, 'es': 775, 'sv': 341, 'hr': 1757, 'fr': 9454, 'pt': 355, 'hsb': 182, 'nl': 2175, 'ro': 228, 'mk': 1811, 'sr': 1722, 'hy': 4533, 'sl': 1624, 'en': 72351, 'be': 15023, 'lv': 2448, 'el': 227, 'bg': 15043, 'uk': 33901, 'lt': 120, 'pl': 24663, 'de': 17708}\n"
     ]
    }
   ],
   "source": [
    "print({ lang: len(l[lang]) for lang in l_arr })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'б', 'x', 'я', 'f', 'х', 'b', 'ž', 'ï', 'н', 'ц', 'v', 'з', 'î', 'ж', 'ê', 'а', 'у', 'ç', 'j', 'ь', 'q', 'ъ', 'ü', 'э', 'd', 'ß', 'á', 'p', 'в', 'њ', 'ч', 'ј', 'w', 'д', 'i', 'п', 'u', 'œ', 'и', 'a', 'l', 'ю', 'c', 'o', 'k', 'к', 'ù', 'm', 'г', 'y', 'й', 'h', 'щ', 'ó', 'о', 'ô', 'ö', 'л', 'ы', 'û', 's', 'р', 'é', 't', 'â', 'ш', 'z', 'n', 'e', 'r', 'g', 'č', 'è', 'с', 'ф', 'е', 'м', 'ä', 'т', 'à'} 80 123 92\n"
     ]
    }
   ],
   "source": [
    "def intersection(i, j):\n",
    "    inter = set(alphabets[i].keys()).intersection(set(alphabets[j].keys()))\n",
    "    print(inter, len(inter), len(alphabets[i]), len(alphabets[j]))\n",
    "    \n",
    "intersection(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def train_data_for_lang_group(group):\n",
    "    data = []\n",
    "    for lang in group:\n",
    "        for sent in l[lang]:\n",
    "            data.append((sent, lang))\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data_for_lang_group(l_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387347\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_chunk(data, begin, length=10000):\n",
    "    return data[begin:begin+length-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sent2vec(sent, alphabet):\n",
    "    vec = []\n",
    "    for c in sent:\n",
    "        v = [0] * len(alphabet)\n",
    "        v[alphabet[c]] = 1\n",
    "        vec.append(np.array(v))\n",
    "    return np.array(vec)\n",
    "\n",
    "def lang2vec(lang, group_lang_ids):\n",
    "    v = [0] * len(group_lang_ids)\n",
    "    v[group_lang_ids[lang]] = 1\n",
    "    return np.array(v)\n",
    "\n",
    "def prepare_data(data, alphabet, group_lang_ids):\n",
    "    x = []\n",
    "    y = []\n",
    "    for sent, lang in data:\n",
    "        if len(sent) == min_len:\n",
    "            x.append(sent2vec(sent, alphabet))\n",
    "            y.append(lang2vec(lang, group_lang_ids))\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk = data_chunk(train_data, 0)\n",
    "\n",
    "x_train, y_train = prepare_data(chunk, A, l_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 132, 215) (132, 215)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, Flatten, Reshape\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Masking, Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "\n",
    "\n",
    "def compile_model(model, learning_rate=0.01, d=1e-6):\n",
    "    sgd = SGD(lr=learning_rate, decay=d) \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_dataset, train_labels, bs=80, epoch=20):\n",
    "    model.fit(train_dataset, train_labels, batch_size=bs, nb_epoch=epoch)\n",
    "\n",
    "def rnn_model(alphabet_len, group_len):\n",
    "    model = Sequential([\n",
    "#         Masking(mask_value=0., input_shape=(seq_len, vec_len)),\n",
    "        LSTM(alphabet_len, input_shape=(132, alphabet_len), return_sequences=True),\n",
    "#         LSTM(50, return_sequences=True),\n",
    "        LSTM(50),\n",
    "        Dense(group_len, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = compile_model(rnn_model(len(A), len(l_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk = data_chunk(train_data, 10000)\n",
    "\n",
    "x_train, y_train = prepare_data(chunk, A, l_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] 6060 51\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_train)):\n",
    "    if len(x_train[i]) != 132:\n",
    "        print (x_train[i], i, len(x_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_model(model, x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
