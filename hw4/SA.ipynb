{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "\n",
    "\n",
    "train_file = csv.reader(open('train.data'), delimiter='\\t')\n",
    "next(train_file)\n",
    "train_set = [x for x in train_file]\n",
    "test_file = csv.reader(open('test.data'), delimiter='\\t')\n",
    "next(test_file)\n",
    "test_set = [x for x in test_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label = [line[2].lower() for line in train_set], [line[1] for line in train_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "Каждое ревью, с помощью токенизации, стемминга и удаления стоп слов, преобразуем в последовательность слов.\n",
    "Кроме того, формируем отдельный список sents, содержащий в себе все предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "sw = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'[\\w\\']+')\n",
    "\n",
    "sents = []\n",
    "for i in range(len(train_data)):\n",
    "    train_data[i] = nltk.sent_tokenize(train_data[i])\n",
    "    for j in range(len(train_data[i])):\n",
    "        tokens = tokenizer.tokenize(train_data[i][j])\n",
    "        train_data[i][j]  = [stemmer.stem(w) for w in tokens if not w in sw]\n",
    "#         train_data[i][j]  = [w for w in tokens if not w in sw]\n",
    "    \n",
    "    words = []\n",
    "    for s in train_data[i]:\n",
    "        sents.append(s)\n",
    "        for w in s:\n",
    "            words.append(w)\n",
    "    train_data[i] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def create_bow_with_freq(data):\n",
    "    result = Counter()\n",
    "    for s in data:\n",
    "        result.update(s)\n",
    "    return list(result.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем частоту каждого слова в обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bow = create_bow_with_freq(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбираем 100 наиболее частотных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 251915),\n",
       " ('place', 99857),\n",
       " ('good', 90131),\n",
       " ('like', 84485),\n",
       " ('food', 80425),\n",
       " ('go', 70479),\n",
       " ('get', 68699),\n",
       " ('time', 67857),\n",
       " ('one', 63277),\n",
       " ('great', 58061)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_word = sorted(train_bow, key=lambda x: x[1], reverse=True)[:100]\n",
    "most_frequent_word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfw_id = {}\n",
    "mfw_set = None\n",
    "\n",
    "i = 0\n",
    "w_lst = []\n",
    "for w,_ in most_frequent_word:\n",
    "    mfw_id[w] = i\n",
    "    i += 1\n",
    "    \n",
    "    w_lst.append(w)\n",
    "\n",
    "mfw_set = set(w_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем idf для выделенных частотных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "mfw_idf = {}\n",
    "\n",
    "for rev in train_data:\n",
    "    r = set(rev)\n",
    "    for w in r:\n",
    "        c = mfw_idf.get(w, 0)\n",
    "        mfw_idf[w] = c + 1\n",
    "\n",
    "for w in mfw_idf:\n",
    "    mfw_idf[w] = log(len(train_data) / mfw_idf[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем всю обучающую выборку на train и validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/austud/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(train_data, train_label, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    return sum(int(y_t == y_p) for y_t, y_p in zip(y_true, y_pred)) * 100 / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.metrics.scores import precision, recall\n",
    "\n",
    "\n",
    "def evaluate_classifier(features_extractor, train, train_l, test, test_l):\n",
    "    \"\"\"\n",
    "         features_extractor - function for extraction features from review. \n",
    "         train, test - samples\n",
    "    \"\"\"\n",
    "    train_feats = [(features_extractor(review), sent) for review, sent in zip(train, train_l)]\n",
    "    test_feats = [(features_extractor(review), sent) for review, sent in zip(test, test_l)]\n",
    " \n",
    "    classifier = NaiveBayesClassifier.train(train_feats)\n",
    "    classifier.show_most_informative_features()\n",
    "    \n",
    "    predictions = []\n",
    "    for i, (feats, label) in enumerate(test_feats):\n",
    "        observed = classifier.classify(feats)\n",
    "        predictions.append(observed)\n",
    "    \n",
    "    return evaluate(test_l, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель 1\n",
    "Пробуем Naive Bayes. Так как каждое ревью - это просто список слов, то признаками будут просто слова входящие в этот список.\n",
    "### Результат на validate: \n",
    "39.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            unprofession = True                1 : 5      =    160.6 : 1.0\n",
      "                incompet = True                1 : 5      =    106.7 : 1.0\n",
      "              disrespect = True                1 : 4      =    100.7 : 1.0\n",
      "                unaccept = True                1 : 5      =     64.0 : 1.0\n",
      "                    ined = True                1 : 5      =     61.5 : 1.0\n",
      "                   appal = True                1 : 5      =     61.1 : 1.0\n",
      "                   crook = True                1 : 4      =     52.5 : 1.0\n",
      "                 unappet = True                1 : 5      =     47.8 : 1.0\n",
      "                  navoid = True                1 : 5      =     47.8 : 1.0\n",
      "               dishonest = True                1 : 4      =     46.8 : 1.0\n",
      "39.2026837260103\n"
     ]
    }
   ],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "val = evaluate_classifier(word_feats, x_train, y_train, x_validate, y_validate)\n",
    "print (val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель 2\n",
    "Попробуем Naive Bayes, но теперь к признакам добавляем еще и популярные биграмы.\n",
    "### Результат на validate:\n",
    "34.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "          ('food', 'ok') = True                2 : 5      =    161.4 : 1.0\n",
      "            unprofession = True                1 : 5      =    160.6 : 1.0\n",
      "                incompet = True                1 : 5      =    106.7 : 1.0\n",
      "              disrespect = True                1 : 4      =    100.7 : 1.0\n",
      "           ('give', '2') = True                2 : 5      =     97.2 : 1.0\n",
      "     ('food', 'mediocr') = True                2 : 5      =     80.2 : 1.0\n",
      "      ('food', 'poison') = True                1 : 5      =     74.8 : 1.0\n",
      "        ('even', 'wors') = True                1 : 5      =     72.4 : 1.0\n",
      "        ('high', 'hope') = True                2 : 5      =     65.1 : 1.0\n",
      "                unaccept = True                1 : 5      =     64.0 : 1.0\n",
      "34.77726634420346\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "\n",
    "def freq_scorer(n_ii, n_ix_xi_tuple, n_xx):\n",
    "    return n_ii / n_xx\n",
    "\n",
    "def bigram_word_feats(words, score_fn=freq_scorer, n=50):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "val = evaluate_classifier(bigram_word_feats, x_train, y_train, x_validate, y_validate)\n",
    "print (val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V\n",
    "Обучим Word2Vec на предложениях из обучающей выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "model = gensim.models.Word2Vec(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rev2vec(rev, vec_len, model):\n",
    "    vec = [0] * (vec_len + 100)\n",
    "    \n",
    "    j = 0\n",
    "    for w in rev:\n",
    "        if w in model:\n",
    "            v = model[w]\n",
    "            j += 1\n",
    "            for i in range(vec_len):\n",
    "                vec[i] += v[i]\n",
    "        if w in mfw_set:\n",
    "            vec[mfw_id[w] + vec_len] = mfw_idf[w]\n",
    "    \n",
    "    for i in range(vec_len):\n",
    "        vec[i] /= j\n",
    "    \n",
    "    return np.array(vec)\n",
    "\n",
    "def prepare2(data, model, vec_len=100):\n",
    "    new_data = []\n",
    "    for r in data:\n",
    "        new_data.append(rev2vec(r,vec_len,model))\n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение обучающей выборки\n",
    "Воспользуемся векторами из Word2Vec.\n",
    "\n",
    "Каждому ревью ставится в соответствие вектор длины 200, где первые 100 элементов - это центроид векторов слов, входящих в данное ревью, оставшиеся 100 - idf для слов, которые входят в ревью и в 100 наиболее частотных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vecs = prepare2(x_train, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модели 3,4\n",
    "Попробуем на построенных векторах обучить SVM (Random Forest).\n",
    "### Результаты на validate:\n",
    "<b>SVM</b>: 52.02<br/>\n",
    "<b>RF</b>: 41.39\n",
    "### Модификации\n",
    "Была предпринята попытка использовать предобученные Word2Vec вектора. И частоту слова в ревью вместо idf.\n",
    "К сожалению, ни по отдельности ни вместе данные модификации не привели к улучшению результата."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "# clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(train_vecs,  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.9815883913247\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(len(x_validate)):\n",
    "    observed = clf.predict([ rev2vec(x_validate[i], 100, model) ])\n",
    "    predictions.append(observed[0])\n",
    "\n",
    "print (evaluate(y_validate, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение обучающей выборки\n",
    "Сделаем так, чтобы все ревью имели одинаковую длину (количество слов). И каждое слово в ревью представим в виде соответствующего ему вектора из Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "rev_len = 80\n",
    "vec_len = 100\n",
    "\n",
    "def prepare(X, model):\n",
    "    for i in range(len(X)):\n",
    "        s = X[i]\n",
    "        words = []\n",
    "        j = 0\n",
    "        for w in s:\n",
    "            if w in model:\n",
    "                if j < rev_len:\n",
    "                    words.append(np.array(model[w]))\n",
    "                    j += 1\n",
    "        \n",
    "        while j < rev_len:\n",
    "            a = np.empty(vec_len)\n",
    "            a.fill(0)\n",
    "            words.append(a)\n",
    "            j += 1\n",
    "        \n",
    "        X[i] = np.array(words)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_outs = 5\n",
    "n_hidden = 80\n",
    "data_len = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = prepare(train_data[:data_len], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_labels(Y):\n",
    "    for i in range(len(Y)):\n",
    "        l = int(Y[i]) - 1\n",
    "        vec = [0] * n_outs\n",
    "        vec[l] = 1\n",
    "        vec = np.array(vec)\n",
    "        Y[i] = vec\n",
    "    return np.array(Y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним one-hot encoding для классов (оценок ревью)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_label = prepare_labels(train_label[:data_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель 5\n",
    "Воспользуемся RNN с тремя LSTM слоями.\n",
    "### Результаты: \n",
    "55.20\n",
    "### Результаты на kaggle:\n",
    "53.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Convolution2D, Flatten, Reshape\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Masking, Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, SimpleRNN, GRU\n",
    "\n",
    "\n",
    "def compile_model(model, learning_rate=0.01, d=1e-6):\n",
    "    sgd = SGD(lr=learning_rate, decay=d) \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, train_dataset, train_labels, bs=80, epoch=20):\n",
    "    model.fit(train_dataset, train_labels, batch_size=bs, nb_epoch=epoch)\n",
    "\n",
    "def rnn_model(seq_len):\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0., input_shape=(seq_len, vec_len)),\n",
    "        LSTM(n_hidden, input_shape=(seq_len, vec_len), return_sequences=True),\n",
    "        LSTM(n_hidden, return_sequences=True),\n",
    "        LSTM(n_hidden),\n",
    "        Dense(n_outs, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2 = compile_model(rnn_model(rev_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 545s - loss: 1.4037 - categorical_accuracy: 0.3733   \n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 554s - loss: 1.2437 - categorical_accuracy: 0.4446   \n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 533s - loss: 1.1842 - categorical_accuracy: 0.4715   \n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 561s - loss: 1.1537 - categorical_accuracy: 0.4848   \n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 598s - loss: 1.1326 - categorical_accuracy: 0.4968   \n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 561s - loss: 1.1176 - categorical_accuracy: 0.5017   \n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 516s - loss: 1.1042 - categorical_accuracy: 0.5099   \n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 519s - loss: 1.0930 - categorical_accuracy: 0.5145   \n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 521s - loss: 1.0832 - categorical_accuracy: 0.5187   \n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 525s - loss: 1.0752 - categorical_accuracy: 0.5219   \n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 521s - loss: 1.0679 - categorical_accuracy: 0.5255   \n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 522s - loss: 1.0591 - categorical_accuracy: 0.5296   \n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 526s - loss: 1.0526 - categorical_accuracy: 0.5330   \n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 528s - loss: 1.0462 - categorical_accuracy: 0.5380   \n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 525s - loss: 1.0410 - categorical_accuracy: 0.5388   \n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 529s - loss: 1.0354 - categorical_accuracy: 0.5409   \n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 538s - loss: 1.0283 - categorical_accuracy: 0.5447   \n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 580s - loss: 1.0242 - categorical_accuracy: 0.5474   \n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 689s - loss: 1.0184 - categorical_accuracy: 0.5500   \n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 647s - loss: 1.0142 - categorical_accuracy: 0.5515   \n"
     ]
    }
   ],
   "source": [
    "train_model(model2, train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_set)):\n",
    "    sent = test_set[i][1]\n",
    "    \n",
    "    sent = nltk.sent_tokenize(sent)\n",
    "    for j in range(len(sent)):\n",
    "        tokens = tokenizer.tokenize(sent[j])\n",
    "        sent[j] = [stemmer.stem(w) for w in tokens if not w in sw]\n",
    "    \n",
    "    words = []\n",
    "    for s in sent:\n",
    "        for w in s:\n",
    "            words.append(w)\n",
    "    test_set[i][1] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_set)):\n",
    "    for j in range(len(test_set[i][1])):\n",
    "        test_set[i][1][j] = test_set[i][1][j].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform(s):\n",
    "    words = []\n",
    "    j = 0\n",
    "    for w in s:\n",
    "        if w in model:\n",
    "            if j < rev_len:\n",
    "                words.append(np.array(model[w]))\n",
    "                j += 1\n",
    "\n",
    "    while j < rev_len:\n",
    "        a = np.empty(vec_len)\n",
    "        a.fill(0)\n",
    "        words.append(a)\n",
    "        j += 1\n",
    "    return np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(s):\n",
    "    x = np.array([transform(s)])\n",
    "    return np.argmax(model2.predict( x )[0]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_out = open('test.out', 'w')\n",
    "writer = csv.writer(test_out)\n",
    "writer.writerow(['ID', 'Sentiment'])\n",
    "for i, s in test_set:\n",
    "    writer.writerow([i, predict(s)])\n",
    "    \n",
    "test_out.flush()\n",
    "test_out.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
